{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_NADACorpus",
      "provenance": [],
      "mount_file_id": "https://github.com/AmeenReda1/Text_classification_NADACorpus/blob/main/Text_Classification_NADACorpus.ipynb",
      "authorship_tag": "ABX9TyMJ3behlXnuJVBBGaZCKC3D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmeenReda1/Text_classification_NADACorpus/blob/main/Text_Classification_NADACorpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iVsr9ibmeeC"
      },
      "source": [
        "#Text_Classification_NADACorpus\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV22VM2WjNGa"
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm,preprocessing,metrics"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNbdra3ujSJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944c9c1a-e138-4980-b004-3a5a3c0001bf"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "dataFrame=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Sampled_File_with_SMOTE.csv')\n",
        "stop=stopwords.words('arabic')\n",
        "print(stop)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFwKrhZVkJ4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e43fa32-4855-4419-8201-c1754a2032db"
      },
      "source": [
        "dataFrame['text']=dataFrame['text'].apply(lambda line: \" \".join(word for word in line.split() if word not in stop))\n",
        "print(dataFrame.head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text                     class\n",
            "0  'غالبيت اصول اعمال باقرب ست رحب دولار اختصارا ...  'الإقتصاد-علوم اجتماعية'\n",
            "1  'اضافت اخير بنك دول مال رئيس اعمال يقلص افق لا...  'الإقتصاد-علوم اجتماعية'\n",
            "2  'مستقره اضافت اخير دول اتفاق اعمال يقلص افق لا...  'الإقتصاد-علوم اجتماعية'\n",
            "3  'اوسط سرق بنك عاما ثان متحده مخبرا لملا الفضول...  'الإقتصاد-علوم اجتماعية'\n",
            "4  'اوسط اطول عاما ثان متحده بان مخبرا لملا اعمال...  'الإقتصاد-علوم اجتماعية'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FpHx97akNlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d290f6b5-cb1e-4ed4-ce2a-0d98f2264f55"
      },
      "source": [
        "train_x, valid_x, train_y, valid_y = train_test_split(dataFrame['text'], dataFrame['class'],test_size=0.2)\n",
        "print(len(set(train_y)))\n",
        "print(len(set(valid_y)))\n",
        "before_encode_valid_y=valid_y\n",
        "# TFIDF feature generation for a maximum of 5000 features\n",
        "print(\"vaild X\",valid_x.head())\n",
        "print(\"vaild X\",valid_y.head())\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
        "print(tfidf_vect)\n",
        "tfidf_vect.fit(dataFrame['text'])\n",
        "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
        "print(xtrain_tfidf.data)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "10\n",
            "vaild X 9389     'نظم مدين مراكش صيت سياح عالميا حفلا فنيا يجمع...\n",
            "8174     'رحبت حرك تحرير وطن فلسطين بتوص قناصل اتحاد او...\n",
            "10538    'نسب اميبيا معو تعتبر اعراض تال اكثر اسهال عد ...\n",
            "7470     'تداعت ذكر لتصل نزار قبان فترحم مرددا قول بدو ...\n",
            "10725    'نسب اميبيا معو تعتبر اعراض تال اكثر اسهال عد ...\n",
            "Name: text, dtype: object\n",
            "vaild X 9389                  'عام- فنون'\n",
            "8174      'السياسة-علوم اجتماعية'\n",
            "10538    'علوم صحية-علوم تطبيقية'\n",
            "7470        'الأدب العربي-أدبيات'\n",
            "10725    'علوم صحية-علوم تطبيقية'\n",
            "Name: class, dtype: object\n",
            "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=5000,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='\\\\w{1,}', tokenizer=None,\n",
            "                use_idf=True, vocabulary=None)\n",
            "[0.18236445 0.05759617 0.04936724 ... 0.17633475 0.03607561 0.06254498]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmsBVwGokQrx"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label,feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    return metrics.accuracy_score(predictions, valid_y),predictions"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2NvOFoNkTx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757e3103-7226-4393-bc25-82e43ea90789"
      },
      "source": [
        "clf = svm.SVC(kernel='linear')\n",
        "#train_model\n",
        "accuracy,predictions=train_model(clf,xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"accuracy: \",accuracy*100)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  99.57918898240244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hns-gzgkYeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a15815d-fe23-47a6-8620-ec19828c541e"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(valid_y,predictions))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       241\n",
            "           1       0.99      1.00      0.99       253\n",
            "           2       0.99      0.99      0.99       244\n",
            "           3       0.99      1.00      0.99       317\n",
            "           4       1.00      1.00      1.00       262\n",
            "           5       1.00      1.00      1.00       257\n",
            "           6       1.00      1.00      1.00       217\n",
            "           7       1.00      1.00      1.00       287\n",
            "           8       1.00      1.00      1.00       276\n",
            "           9       0.99      0.99      0.99       260\n",
            "\n",
            "    accuracy                           1.00      2614\n",
            "   macro avg       1.00      1.00      1.00      2614\n",
            "weighted avg       1.00      1.00      1.00      2614\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMR_cEpkwoIq",
        "outputId": "57cc61b2-9a8a-46f4-83c8-6e1b984f0199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "disp = plot_confusion_matrix(clf, xvalid_tfidf, valid_y,\n",
        "                                 cmap=plt.cm.Blues,\n",
        "                             labels=['10','11','12','13','14','15','16','17','18','19'])\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:276: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if np.all([l not in y_true for l in labels]):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-33c4887a2cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m disp = plot_confusion_matrix(clf, xvalid_tfidf, valid_y,\n\u001b[1;32m      4\u001b[0m                                  \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                              labels=['10','11','12','13','14','15','16','17','18','19'])\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_plot/confusion_matrix.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(estimator, X, y_true, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight,\n\u001b[0;32m--> 187\u001b[0;31m                           labels=labels, normalize=normalize)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdisplay_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one label specified must be in y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: At least one label specified must be in y_true"
          ]
        }
      ]
    }
  ]
}